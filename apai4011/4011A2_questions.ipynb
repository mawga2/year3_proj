{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3sWPwqK_G9B"
      },
      "source": [
        "## APAI/STAT 4011 Natural Language Processing\n",
        "\n",
        "## Assignment 2\n",
        "\n",
        "### Submission format: 2 files (please don't zip them together), one is the ipynb file implemented with code and comments here, and one is pdf/ html file generated from this notebook. It's highly suggested that you directly write in this notebook and submit a pdf file.\n",
        "\n",
        "*The late submission policy*: If you have difficulty handing in on time (e.g., illness etc.), you would need to send the official certificate to Dr. Lau (and cc the tutor) at least one day before the deadline via email.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPWYlhiLa0aH"
      },
      "source": [
        "## Q1. (40 marks)\n",
        "\n",
        "In Q1, we will be focusing on the IMDb dataset. This is a dataset for binary sentiment classification, and is provided with a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. To load the dataset, you can easily download the dataset by adding this line in your colab notebook:\n",
        "\n",
        "```\n",
        "! wget http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BENL0KG2-PRo",
        "outputId": "ed98d464-0694-4571-f789-34b226d350e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-30 02:32:28--  http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  3.10MB/s    in 29s     \n",
            "\n",
            "2024-11-30 02:32:57 (2.74 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the Large IMDB Movie Review Dataset\n",
        "# the task is binary classification: positive or negative review\n",
        "\n",
        "! wget http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2p2lZKR9-PRq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import os\n",
        "from collections import namedtuple\n",
        "\n",
        "seed = 4011\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_path = \"aclImdb/train/\"\n",
        "test_path = \"aclImdb/test/\"\n",
        "\n",
        "# Hyperparameters for tuning\n",
        "\n",
        "batch_size = 100\n",
        "max_len = 300\n",
        "embedding_size = 300\n",
        "min_count = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um0SqlKu-PRq",
        "outputId": "751ffbaa-772f-4087-d356-828f749947de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing aclImdb/train/pos/: 100%|██████████| 12500/12500 [00:01<00:00, 7978.80it/s]\n",
            "Processing aclImdb/train/neg/: 100%|██████████| 12500/12500 [00:00<00:00, 12798.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([280619, 100])\n"
          ]
        }
      ],
      "source": [
        "###2. Build an appropriate embedding matrix based on the vocabulary and print out the size of this matrix.\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Tokenize and build vocabulary\n",
        "def build_vocab(paths):\n",
        "    counter = Counter()\n",
        "    for path in paths:\n",
        "        for fname in tqdm(os.listdir(path), desc=f\"Processing {path}\"):\n",
        "            with open(os.path.join(path, fname), 'r', encoding='utf-8') as f:\n",
        "                tokens = f.read().strip().split()  # Simple whitespace tokenization\n",
        "                counter.update(tokens)\n",
        "    return counter\n",
        "\n",
        "train_pos = \"aclImdb/train/pos/\"\n",
        "train_neg = \"aclImdb/train/neg/\"\n",
        "counter = build_vocab([train_pos, train_neg])\n",
        "\n",
        "# Build word-to-index dictionary\n",
        "special_tokens = ['<unk>', '<pad>']\n",
        "word_to_idx = {word: idx for idx, (word, _) in enumerate(counter.most_common(), start=len(special_tokens))}\n",
        "word_to_idx.update({tok: i for i, tok in enumerate(special_tokens)})\n",
        "\n",
        "embedding_size = 100\n",
        "vocab_size = len(word_to_idx)\n",
        "embeddings = nn.Embedding(\n",
        "    vocab_size,\n",
        "    embedding_size,\n",
        "    padding_idx=word_to_idx['<pad>']\n",
        ")\n",
        "\n",
        "print(embeddings.weight.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fptpRvy-PRr",
        "outputId": "5bea7764-f54d-4926-8edd-556de8691a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unknown vocab size: 424424\n",
            "Vocab size: 22521\n",
            "Vocab size: 2\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "# 25000 train and 25000 test sentences\n",
        "\n",
        "\n",
        "###After loading the dataset, we will need to perform preprocessing (e.g. tokenization, build up vocabulary, etc.) on the text. We will set the minimum token frequency threshold to be 10. Then print out the size of your vocabulary.\n",
        "#- Special notes: need some special tokens like `<UNK>`, `<PAD>`, `<BOS>`, `<EOS>`. `<UNK>` represents the tokens\n",
        "#    that can not be found in our vocabulary. (Why do we need it?)\n",
        "#    `<PAD>` means padding, and `<BOS>` and `<EOS>` represents beginning-of-sentence and end-of-sentence, respectively.\n",
        "\n",
        "Sentence = namedtuple('Sentence', ['index', 'tokens', 'label'])\n",
        "\n",
        "def read_imdb_movie_dataset(dataset_path):\n",
        "\n",
        "    indices = []\n",
        "    text = []\n",
        "    rating = []\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for filename in os.listdir(os.path.join(dataset_path, \"pos\")):\n",
        "        file_path = os.path.join(dataset_path, \"pos\", filename)\n",
        "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\n",
        "        indices.append(i)\n",
        "        text.append(data)\n",
        "        rating.append(1)\n",
        "        i = i + 1\n",
        "\n",
        "    for filename in os.listdir(os.path.join(dataset_path, \"neg\")):\n",
        "        file_path = os.path.join(dataset_path, \"neg\", filename)\n",
        "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\n",
        "        indices.append(i)\n",
        "        text.append(data)\n",
        "        rating.append(0)\n",
        "        i = i + 1\n",
        "\n",
        "    sentences = [ Sentence(index, text.split(), rating)\n",
        "                  for index, text, rating in zip(indices, text, rating)]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "train_examples = read_imdb_movie_dataset(train_path)\n",
        "test_examples = read_imdb_movie_dataset(test_path)\n",
        "\n",
        "UNK = '<UNK>'\n",
        "PAD = '<PAD>'\n",
        "BOS = '<BOS>'\n",
        "EOS = '<EOS>'\n",
        "\n",
        "\n",
        "class VocabItem:\n",
        "\n",
        "    def __init__(self, string, hash=None):\n",
        "        self.string = string\n",
        "        self.count = 0\n",
        "        self.hash = hash\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'VocabItem({})'.format(self.string)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        min_count=0,\n",
        "        no_unk=False,\n",
        "        add_padding=False,\n",
        "        add_bos=False,\n",
        "        add_eos=False,\n",
        "        unk=None):\n",
        "\n",
        "        self.no_unk = no_unk\n",
        "        self.vocab_items = []\n",
        "        self.vocab_hash = {}\n",
        "        self.word_count = 0\n",
        "        self.special_tokens = []\n",
        "        self.min_count = min_count\n",
        "        self.add_padding = add_padding\n",
        "        self.add_bos = add_bos\n",
        "        self.add_eos = add_eos\n",
        "        self.unk = unk\n",
        "\n",
        "        self.UNK = None\n",
        "        self.PAD = None\n",
        "        self.BOS = None\n",
        "        self.EOS = None\n",
        "\n",
        "        self.index2token = []\n",
        "        self.token2index = {}\n",
        "\n",
        "        self.finished = False\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        if self.finished:\n",
        "            raise RuntimeError('Vocabulary is finished')\n",
        "\n",
        "        for token in tokens:\n",
        "            if token not in self.vocab_hash:\n",
        "                self.vocab_hash[token] = len(self.vocab_items)\n",
        "                self.vocab_items.append(VocabItem(token))\n",
        "\n",
        "            self.vocab_items[self.vocab_hash[token]].count += 1\n",
        "            self.word_count += 1\n",
        "\n",
        "    def finish(self):\n",
        "\n",
        "        token2index = self.token2index\n",
        "        index2token = self.index2token\n",
        "\n",
        "        tmp = []\n",
        "\n",
        "        if not self.no_unk:\n",
        "\n",
        "            # we add/handle the special `UNK` token\n",
        "            # and set it to have index 0 in our mapping\n",
        "            if self.unk:\n",
        "                self.UNK = VocabItem(self.unk, hash=0)\n",
        "                self.UNK.count = self.vocab_items[self.vocab_hash[self.unk]].count\n",
        "                index2token.append(self.UNK)\n",
        "                self.special_tokens.append(self.UNK)\n",
        "\n",
        "                for token in self.vocab_items:\n",
        "                    if token.string != self.unk:\n",
        "                        tmp.append(token)\n",
        "\n",
        "            else:\n",
        "                self.UNK = VocabItem(UNK, hash=0)\n",
        "                index2token.append(self.UNK)\n",
        "                self.special_tokens.append(self.UNK)\n",
        "\n",
        "                for token in self.vocab_items:\n",
        "                    if token.count <= self.min_count:\n",
        "                        self.UNK.count += token.count\n",
        "                    else:\n",
        "                        tmp.append(token)\n",
        "        else:\n",
        "            for token in self.vocab_items:\n",
        "                tmp.append(token)\n",
        "\n",
        "        tmp.sort(key=lambda token: token.count, reverse=True)\n",
        "\n",
        "        if self.add_bos:\n",
        "            self.BOS = VocabItem(BOS)\n",
        "            tmp.append(self.BOS)\n",
        "            self.special_tokens.append(self.BOS)\n",
        "\n",
        "        if self.add_eos:\n",
        "            self.EOS = VocabItem(EOS)\n",
        "            tmp.append(self.EOS)\n",
        "            self.special_tokens.append(self.EOS)\n",
        "\n",
        "        if self.add_padding:\n",
        "            self.PAD = VocabItem(PAD)\n",
        "            tmp.append(self.PAD)\n",
        "            self.special_tokens.append(self.PAD)\n",
        "\n",
        "        index2token += tmp\n",
        "\n",
        "        for i, token in enumerate(self.index2token):\n",
        "            token2index[token.string] = i\n",
        "            token.hash = i\n",
        "\n",
        "        self.index2token = index2token\n",
        "        self.token2index = token2index\n",
        "\n",
        "        if not self.no_unk:\n",
        "            print('Unknown vocab size:', self.UNK.count)\n",
        "\n",
        "        print('Vocab size: %d' % len(self))\n",
        "\n",
        "        self.finished = True\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.index2token[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index2token)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.index2token)\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return key in self.token2index\n",
        "\n",
        "    def tokens2indices(self, tokens, add_bos=False, add_eos=False):\n",
        "        string_seq = []\n",
        "        if add_bos:\n",
        "            string_seq.append(self.BOS.hash)\n",
        "        for token in tokens:\n",
        "            if self.no_unk:\n",
        "                string_seq.append(self.token2index[token])\n",
        "            else:\n",
        "                string_seq.append(self.token2index.get(token, self.UNK.hash))\n",
        "        if add_eos:\n",
        "            string_seq.append(self.EOS.hash)\n",
        "        return string_seq\n",
        "\n",
        "    def indices2tokens(self, indices, ignore_ids=()):\n",
        "        tokens = []\n",
        "        for idx in indices:\n",
        "            if idx in ignore_ids:\n",
        "                continue\n",
        "            tokens.append(self.index2token[idx].string)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "src_vocab = Vocab(min_count=min_count, add_padding=True)\n",
        "\n",
        "tgt_vocab = Vocab(no_unk=True, add_padding=False)\n",
        "\n",
        "for sentence in train_examples:\n",
        "    src_vocab.add_tokens(sentence.tokens[:max_len])\n",
        "    tgt_vocab.add_tokens([sentence.label])\n",
        "\n",
        "src_vocab.finish()\n",
        "tgt_vocab.finish()\n",
        "\n",
        "\n",
        "Vocabs = namedtuple('Vocabs', ['src', 'tgt'])\n",
        "vocabs = Vocabs(src_vocab, tgt_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHMHTUkg-PRs"
      },
      "source": [
        "\n",
        "### Q1-1. To get your data prepared, build up Pytorch dataloaders for model training and print out one batch of training data. (15 marks)\n",
        "\n",
        "- To check whether your dataloader can work successfully, you can choose to use `next(iter(train_dataloader))`. You can refer to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7NsOBAZx-PRs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "###To get your data prepared, build up Pytorch dataloaders for model training and print out one batch of training data.\n",
        "#- To check whether your dataloader can work successfully, you can choose to use `next(iter(train_dataloader))`. You can refer to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html.\n",
        "\n",
        "\n",
        "# The Batch objects\n",
        "# To easily access all the data in a batch, let's create a special Batch object that will give us access to\n",
        "# all the information we may require during training.\n",
        "# Let's begin creating a more friendly object that contains a numeric representation of our inputs and outputs.\n",
        "# By default we will use numpy objects, but we will also add a function to translate the contents of the object to PyTorch.\n",
        "# We will create this object to be generic enough so we can use it with tasks other than classification, too.\n",
        "# This object will work like a dictionary,\n",
        "# but it will also allow us to access each component using an attribute with the same name.\n",
        "# The main principle is that this dictionary-like batch will hold `numpy` objects as values,\n",
        "# and that after calling the `to_torch_()` function, they will be turned into `pytorch` objects and moved to\n",
        "# the corresponding provided device.\n",
        "# In this way, we know that all our elements inside the batch object are in the right place.\n",
        "# We will combine our `Batch` object with a `BatchTuple` object that will hold data relevant to a specific input of the model.\n",
        "\n",
        "class Batch(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Batch, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        self._is_torch = False\n",
        "\n",
        "    def to_torch_(self, device):\n",
        "        self._is_torch = False\n",
        "        for key in self.keys():\n",
        "            value = self[key]\n",
        "            # we move `numpy` objects to `pytorch`\n",
        "            if isinstance(value, BatchTuple):\n",
        "                value.to_torch_(device)\n",
        "            # we also move our BatchTuple objects to `pytorch`\n",
        "            if isinstance(value, np.ndarray):\n",
        "                self[key] = torch.from_numpy(value).to(device)\n",
        "\n",
        "\n",
        "class BatchTuple(object):\n",
        "    def __init__(self, sequences, lengths, sublengths, masks):\n",
        "        self.sequences = sequences\n",
        "        self.lengths = lengths\n",
        "        self.sublengths = sublengths\n",
        "        self.masks = masks\n",
        "        self._is_torch = False\n",
        "\n",
        "    def to_torch_(self, device):\n",
        "        if not self._is_torch:\n",
        "            self.sequences = torch.tensor(\n",
        "                self.sequences, device=device, dtype=torch.long\n",
        "            )\n",
        "\n",
        "            if self.lengths is not None:\n",
        "                self.lengths = torch.tensor(\n",
        "                    self.lengths, device=device, dtype=torch.long\n",
        "                )\n",
        "\n",
        "            if self.sublengths is not None:\n",
        "                self.sublengths = torch.tensor(\n",
        "                    self.sublengths, device=device, dtype=torch.long\n",
        "                )\n",
        "            if self.masks is not None:\n",
        "                self.masks = torch.tensor(\n",
        "                    self.masks, device=device, dtype=torch.float\n",
        "                )\n",
        "\n",
        "\n",
        "# The padding function\n",
        "\n",
        "def pad_list(\n",
        "    sequences,\n",
        "    dim0_pad=None,\n",
        "    dim1_pad=None,\n",
        "    align_right=False,\n",
        "    pad_value=0\n",
        "):\n",
        "\n",
        "    sequences = [np.asarray(sublist) for sublist in sequences]\n",
        "\n",
        "    if not dim0_pad:\n",
        "        dim0_pad = len(sequences)\n",
        "\n",
        "    if not dim1_pad:\n",
        "        dim1_pad = max(len(seq) for seq in sequences)\n",
        "\n",
        "    out = np.full(shape=(dim0_pad, dim1_pad), fill_value=pad_value)\n",
        "\n",
        "    lengths = []\n",
        "    for i in range(len(sequences)):\n",
        "        data_length = len(sequences[i])\n",
        "        lengths.append(data_length)\n",
        "        offset = dim1_pad - data_length if align_right else 0\n",
        "        np.put(out[i], range(offset, offset + data_length), sequences[i])\n",
        "\n",
        "    lengths = np.array(lengths)\n",
        "\n",
        "    return out, lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3WYrLF9e-PRt"
      },
      "outputs": [],
      "source": [
        "#-------------------\n",
        "# Write your code\n",
        "\n",
        "class SequenceClassificationBatchBuilder(object):\n",
        "    def __init__(self, vocabs, max_len=None):\n",
        "        self.vocabs = vocabs\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, examples):\n",
        "      ids_batch = [int(sentence.index) for sentence in examples]\n",
        "\n",
        "      src_examples = [\n",
        "          self.vocabs.src.tokens2indices(sentence.tokens[: self.max_len])\n",
        "          for sentence in examples\n",
        "      ]\n",
        "\n",
        "      tgt_examples = [\n",
        "          self.vocabs.tgt.token2index[sentence.label] for sentence in examples\n",
        "      ]\n",
        "\n",
        "      src_padded, src_lengths = pad_list(\n",
        "          src_examples, pad_value=self.vocabs.src.PAD.hash\n",
        "      )\n",
        "\n",
        "      src_batch_tuple = BatchTuple(src_padded, src_lengths, None, None)\n",
        "\n",
        "      tgt_batch_tuple = BatchTuple(tgt_examples, None, None, None)\n",
        "\n",
        "      return Batch(\n",
        "          indices=ids_batch, src=src_batch_tuple, tgt=tgt_batch_tuple\n",
        "      )\n",
        "\n",
        "# Let's instance our `batch_builder`, feed it into the `DataLoader` object alongside the  training and test examples,\n",
        "# and let's inspect a single batch of examples.\n",
        "batch_builder = SequenceClassificationBatchBuilder(\n",
        "    vocabs, max_len=max_len\n",
        ")\n",
        "\n",
        "train_batches = DataLoader(\n",
        "    train_examples,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=batch_builder,\n",
        ")\n",
        "\n",
        "test_batches = DataLoader(\n",
        "    test_examples,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=batch_builder,\n",
        ")\n",
        "#----------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches_iter = iter(train_batches)\n",
        "train_batch = next(train_batches_iter)\n",
        "train_batch.src.sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-C6lFLq7KA3",
        "outputId": "44a1aa58-2214-41be-e5e8-26529eca9091"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    8,   177,   202, ...,    92,    11,    48],\n",
              "       [  240,    21,     1, ..., 22520, 22520, 22520],\n",
              "       [  994,  6211,   207, ..., 22520, 22520, 22520],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,  2227,     9,  1738],\n",
              "       [   17,   457,     7, ..., 22520, 22520, 22520],\n",
              "       [   17,  4196,   815, ..., 22520, 22520, 22520]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQo4U_k-PRt"
      },
      "source": [
        "### Q1-2. We choose bidirectional LSTM (BiLSTM) as the model. Train the model for 5 epoches with embedding matrix you obtained earlier, and for each epoch, print out the training loss, training accuracy, testing loss and testing accuracy. You could choose any appropriate loss function and values for hyperparameters. (25 marks)\n",
        "\n",
        "- If you found difficulty understanding the structure of BiLSTM, you may refer to the supplementary note named *notes_on_lstm* inside tutorial 9 for detailed information.\n",
        "\n",
        "- You definitely want to use GPU for this colab notebook. Go to Edit > Notebook settings as the following: Click on “Notebook settings” and select “GPU”."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(batch_hidden_states, batch_lengths):\n",
        "    batch_lengths = batch_lengths.float()\n",
        "    batch_lengths = batch_lengths.unsqueeze(1)\n",
        "    if batch_hidden_states.is_cuda:\n",
        "        batch_lengths = batch_lengths.cuda()\n",
        "\n",
        "    pooled_batch = torch.sum(batch_hidden_states, 1)\n",
        "    pooled_batch = pooled_batch / batch_lengths.expand_as(pooled_batch)\n",
        "\n",
        "    return pooled_batch\n",
        "\n",
        "\n",
        "def max_pooling(batch_hidden_states):\n",
        "    pooled_batch, _ = torch.max(batch_hidden_states, 1)\n",
        "    return pooled_batch\n",
        "\n",
        "def pack_rnn_input(embedded_sequence_batch, sequence_lengths):\n",
        "    sequence_lengths = sequence_lengths.cpu().numpy()\n",
        "\n",
        "    sorted_sequence_lengths = np.sort(sequence_lengths)[::-1]\n",
        "    sorted_sequence_lengths = torch.from_numpy(\n",
        "        sorted_sequence_lengths.copy()\n",
        "    )\n",
        "\n",
        "    idx_sort = np.argsort(-sequence_lengths)\n",
        "    idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "    idx_sort = torch.from_numpy(idx_sort)\n",
        "    idx_unsort = torch.from_numpy(idx_unsort)\n",
        "\n",
        "    if embedded_sequence_batch.is_cuda:\n",
        "        idx_sort = idx_sort.cuda()\n",
        "        idx_unsort = idx_unsort.cuda()\n",
        "\n",
        "    embedded_sequence_batch = embedded_sequence_batch.index_select(\n",
        "        0, idx_sort\n",
        "    )\n",
        "\n",
        "    # Handling padding in Recurrent Networks\n",
        "    packed_rnn_input = nn.utils.rnn.pack_padded_sequence(\n",
        "        embedded_sequence_batch,\n",
        "        sorted_sequence_lengths,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    return packed_rnn_input, idx_unsort\n",
        "\n",
        "def unpack_rnn_output(packed_rnn_output, indices):\n",
        "    encoded_sequence_batch, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "        packed_rnn_output, batch_first=True\n",
        "    )\n",
        "\n",
        "    encoded_sequence_batch = encoded_sequence_batch.index_select(0, indices)\n",
        "\n",
        "    return encoded_sequence_batch\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embeddings, hidden_size, num_labels, input_dropout=0, output_dropout=0, bidirectional=True, num_layers=2, pooling='mean'):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.pooling = pooling\n",
        "        self.input_dropout = nn.Dropout(input_dropout)\n",
        "        self.output_dropout = nn.Dropout(output_dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_layers = num_layers\n",
        "        self.num_labels = num_labels\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = self.embeddings.embedding_dim\n",
        "        self.lstm = nn.LSTM(self.input_size, hidden_size, bidirectional=bidirectional, num_layers=num_layers, batch_first=True)\n",
        "        self.total_hidden_size = self.hidden_size * (2 if self.bidirectional else 1)\n",
        "        self.output_layer = nn.Linear(self.total_hidden_size, self.num_labels)\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, src_batch, tgt_batch=None):\n",
        "        src_sequences = src_batch.sequences\n",
        "        src_lengths = src_batch.lengths\n",
        "\n",
        "        embedded_sequence_batch = self.embeddings(src_sequences)\n",
        "        embedded_sequence_batch = self.input_dropout(embedded_sequence_batch)\n",
        "\n",
        "        packed_rnn_input, indices = pack_rnn_input(embedded_sequence_batch, src_lengths)\n",
        "        rnn_packed_output, _ = self.lstm(packed_rnn_input)\n",
        "        encoded_sequence_batch = unpack_rnn_output(rnn_packed_output, indices)\n",
        "\n",
        "        if self.pooling == \"mean\":\n",
        "            pooled_batch = mean_pooling(encoded_sequence_batch, src_lengths)\n",
        "        elif self.pooling == \"max\":\n",
        "            pooled_batch = max_pooling(encoded_sequence_batch)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        logits = self.output_layer(pooled_batch)\n",
        "        _, predictions = logits.max(1)\n",
        "\n",
        "        if tgt_batch is not None:\n",
        "            targets = tgt_batch.sequences\n",
        "            loss = self.loss_function(logits, targets)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return loss, predictions, logits"
      ],
      "metadata": {
        "id": "2vErn1f9W72D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "hidden_size = 300\n",
        "log_interval = 10\n",
        "num_labels = 2\n",
        "input_dropout = 0.5\n",
        "output_dropout = 0.5\n",
        "bidirectional = True\n",
        "num_layers = 2\n",
        "pooling = 'mean'\n",
        "lr = 0.001\n",
        "gradient_clipping = 0.25\n",
        "\n",
        "model = BiLSTM(\n",
        "    embeddings=embeddings,\n",
        "    hidden_size=hidden_size,\n",
        "    num_labels=num_labels,\n",
        "    input_dropout=input_dropout,\n",
        "    output_dropout=output_dropout,\n",
        "    bidirectional=bidirectional,\n",
        "    num_layers=num_layers,\n",
        "    pooling=pooling\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_batches:\n",
        "        batch.to_torch_(device)\n",
        "        src_batch = batch.src\n",
        "        tgt_batch = batch.tgt\n",
        "\n",
        "        loss, predictions, logits = model(src_batch, tgt_batch=tgt_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(),\n",
        "            gradient_clipping)\n",
        "\n",
        "        optimizer.step()\n",
        "        correct = (predictions == tgt_batch.sequences).long().sum()\n",
        "        total = tgt_batch.sequences.size(0)\n",
        "        epoch_correct += correct.item()\n",
        "        epoch_total += total\n",
        "        epoch_loss += loss.item()\n",
        "        i += 1\n",
        "\n",
        "    accuracy  = 100 * epoch_correct / epoch_total\n",
        "\n",
        "    print('Epoch {}'.format(epoch))\n",
        "    print('Train Loss: {}'.format(epoch_loss / len(train_batches)))\n",
        "    print('Train Accuracy: {}'.format(accuracy))\n",
        "\n",
        "    test_epoch_correct = 0\n",
        "    test_epoch_total = 0\n",
        "    test_epoch_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch in test_batches:\n",
        "\n",
        "        ids_batch = batch.indices\n",
        "        src_batch = batch.src\n",
        "        tgt_batch = batch.tgt\n",
        "\n",
        "        batch.to_torch_(device)\n",
        "\n",
        "        loss, predictions, logits = model.forward(\n",
        "            src_batch,\n",
        "            tgt_batch=tgt_batch)\n",
        "\n",
        "        correct = (predictions == tgt_batch.sequences).long().sum()\n",
        "        total = tgt_batch.sequences.size(0)\n",
        "        test_epoch_correct += correct.item()\n",
        "        test_epoch_total += total\n",
        "        test_epoch_loss += loss.item()\n",
        "\n",
        "    test_accuracy = 100 * test_epoch_correct / test_epoch_total\n",
        "\n",
        "    print('\\n---------------------')\n",
        "    print('Test Loss: {}'.format(test_epoch_loss / len(test_batches)))\n",
        "    print('Test Accuracy: {}'.format(test_accuracy))\n",
        "    print('---------------------\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8GTVju7EMVD",
        "outputId": "7c8cfbc3-f3d3-4284-f369-465ba8ce0c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Train Loss: 0.6211941704750061\n",
            "Train Accuracy: 63.968\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.45040971946716307\n",
            "Test Accuracy: 79.236\n",
            "---------------------\n",
            "\n",
            "Epoch 1\n",
            "Train Loss: 0.45934455215930936\n",
            "Train Accuracy: 77.808\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.41602176943421365\n",
            "Test Accuracy: 80.764\n",
            "---------------------\n",
            "\n",
            "Epoch 2\n",
            "Train Loss: 0.37459966629743574\n",
            "Train Accuracy: 83.244\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.3302415445446968\n",
            "Test Accuracy: 86.256\n",
            "---------------------\n",
            "\n",
            "Epoch 3\n",
            "Train Loss: 0.31761866080760953\n",
            "Train Accuracy: 86.276\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.3459793331623077\n",
            "Test Accuracy: 85.676\n",
            "---------------------\n",
            "\n",
            "Epoch 4\n",
            "Train Loss: 0.273804653942585\n",
            "Train Accuracy: 88.744\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.301221303999424\n",
            "Test Accuracy: 87.864\n",
            "---------------------\n",
            "\n",
            "Epoch 5\n",
            "Train Loss: 0.22896914321184159\n",
            "Train Accuracy: 90.864\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.3400253424048424\n",
            "Test Accuracy: 87.396\n",
            "---------------------\n",
            "\n",
            "Epoch 6\n",
            "Train Loss: 0.20002151882648467\n",
            "Train Accuracy: 91.864\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.360567166864872\n",
            "Test Accuracy: 87.956\n",
            "---------------------\n",
            "\n",
            "Epoch 7\n",
            "Train Loss: 0.18560611727833748\n",
            "Train Accuracy: 92.656\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.389529370829463\n",
            "Test Accuracy: 86.684\n",
            "---------------------\n",
            "\n",
            "Epoch 8\n",
            "Train Loss: 0.1640427635461092\n",
            "Train Accuracy: 93.632\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.43027725237607956\n",
            "Test Accuracy: 86.46\n",
            "---------------------\n",
            "\n",
            "Epoch 9\n",
            "Train Loss: 0.14357198686152697\n",
            "Train Accuracy: 94.408\n",
            "\n",
            "---------------------\n",
            "Test Loss: 0.47566260582208636\n",
            "Test Accuracy: 85.848\n",
            "---------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(batch_hidden_states, batch_lengths):\n",
        "    batch_lengths = batch_lengths.unsqueeze(1)  # Shape: [batch_size, 1]\n",
        "    return torch.sum(batch_hidden_states, dim=1) / batch_lengths\n",
        "\n",
        "def max_pooling(batch_hidden_states):\n",
        "    return torch.max(batch_hidden_states, 1).values\n",
        "\n",
        "def pack_rnn_input(embedded_batch, lengths):\n",
        "    lengths_sorted, indices_sorted = lengths.sort(descending=True)\n",
        "    embedded_sorted = embedded_batch.index_select(0, indices_sorted)\n",
        "    packed_input = nn.utils.rnn.pack_padded_sequence(embedded_sorted, lengths_sorted.cpu(), batch_first=True)\n",
        "    return packed_input, indices_sorted.argsort()\n",
        "\n",
        "def unpack_rnn_output(packed_output, unsort_indices):\n",
        "    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "    return output.index_select(0, unsort_indices)\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embeddings, hidden_size, num_labels, input_dropout=0, output_dropout=0, bidirectional=True, num_layers=2, pooling='mean'):\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.pooling = pooling\n",
        "        self.lstm = nn.LSTM(embeddings.embedding_dim, hidden_size, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.input_dropout = nn.Dropout(input_dropout)\n",
        "        self.output_dropout = nn.Dropout(output_dropout)\n",
        "        self.output_layer = nn.Linear(hidden_size * (2 if bidirectional else 1), num_labels)\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, src_batch, tgt_batch=None):\n",
        "        embedded = self.input_dropout(self.embeddings(src_batch.sequences))\n",
        "        packed_input, unsort_indices = pack_rnn_input(embedded, src_batch.lengths)\n",
        "        packed_output, _ = self.lstm(packed_input)\n",
        "        output = unpack_rnn_output(packed_output, unsort_indices)\n",
        "\n",
        "        pooled_output = mean_pooling(output, src_batch.lengths) if self.pooling == 'mean' else max_pooling(output)\n",
        "        pooled_output = self.output_dropout(pooled_output)\n",
        "        logits = self.output_layer(pooled_output)\n",
        "        loss = self.loss_function(logits, tgt_batch.sequences) if tgt_batch else None\n",
        "        predictions = logits.argmax(1)\n",
        "\n",
        "        return loss, predictions, logits"
      ],
      "metadata": {
        "id": "XITE5qd-6LDj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "hidden_size = 300\n",
        "log_interval = 10\n",
        "num_labels = 2\n",
        "input_dropout = 0.5\n",
        "output_dropout = 0.5\n",
        "bidirectional = True\n",
        "num_layers = 2\n",
        "pooling = 'mean'\n",
        "lr = 0.001\n",
        "gradient_clipping = 0.25\n",
        "\n",
        "model = BiLSTM(\n",
        "    embeddings=embeddings,\n",
        "    hidden_size=hidden_size,\n",
        "    num_labels=num_labels,\n",
        "    input_dropout=input_dropout,\n",
        "    output_dropout=output_dropout,\n",
        "    bidirectional=bidirectional,\n",
        "    num_layers=num_layers,\n",
        "    pooling=pooling\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_batches:\n",
        "        batch.to_torch_(device)\n",
        "        src_batch = batch.src\n",
        "        tgt_batch = batch.tgt\n",
        "\n",
        "        loss, predictions, logits = model(src_batch, tgt_batch=tgt_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(),\n",
        "            gradient_clipping)\n",
        "\n",
        "        optimizer.step()\n",
        "        correct = (predictions == tgt_batch.sequences).long().sum()\n",
        "        total = tgt_batch.sequences.size(0)\n",
        "        epoch_correct += correct.item()\n",
        "        epoch_total += total\n",
        "        epoch_loss += loss.item()\n",
        "        i += 1\n",
        "\n",
        "    accuracy  = 100 * epoch_correct / epoch_total\n",
        "\n",
        "    print('Epoch {}'.format(epoch))\n",
        "    print('Train Loss: {}'.format(epoch_loss / len(train_batches)))\n",
        "    print('Train Accuracy: {}'.format(accuracy))\n",
        "\n",
        "    test_epoch_correct = 0\n",
        "    test_epoch_total = 0\n",
        "    test_epoch_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch in test_batches:\n",
        "\n",
        "        ids_batch = batch.indices\n",
        "        src_batch = batch.src\n",
        "        tgt_batch = batch.tgt\n",
        "\n",
        "        batch.to_torch_(device)\n",
        "\n",
        "        loss, predictions, logits = model.forward(\n",
        "            src_batch,\n",
        "            tgt_batch=tgt_batch)\n",
        "\n",
        "        correct = (predictions == tgt_batch.sequences).long().sum()\n",
        "        total = tgt_batch.sequences.size(0)\n",
        "        test_epoch_correct += correct.item()\n",
        "        test_epoch_total += total\n",
        "        test_epoch_loss += loss.item()\n",
        "\n",
        "    test_accuracy = 100 * test_epoch_correct / test_epoch_total\n",
        "\n",
        "    print('Test Loss: {}'.format(test_epoch_loss / len(test_batches)))\n",
        "    print('Test Accuracy: {}'.format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0bkFX5O6Ssl",
        "outputId": "60a42085-028b-4a83-c003-027d6e4d8368"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Train Loss: 0.3280042085647583\n",
            "Train Accuracy: 85.592\n",
            "Test Loss: 0.3543203083872795\n",
            "Test Accuracy: 84.696\n",
            "Epoch 1\n",
            "Train Loss: 0.24977229115366936\n",
            "Train Accuracy: 89.792\n",
            "Test Loss: 0.33911906656622887\n",
            "Test Accuracy: 86.684\n",
            "Epoch 2\n",
            "Train Loss: 0.22109418520331384\n",
            "Train Accuracy: 90.964\n",
            "Test Loss: 0.38633763824403283\n",
            "Test Accuracy: 85.132\n",
            "Epoch 3\n",
            "Train Loss: 0.20376477387547492\n",
            "Train Accuracy: 91.924\n",
            "Test Loss: 0.4107707781791687\n",
            "Test Accuracy: 84.32\n",
            "Epoch 4\n",
            "Train Loss: 0.18492900213599206\n",
            "Train Accuracy: 92.66\n",
            "Test Loss: 0.40320356205105784\n",
            "Test Accuracy: 85.832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Zsk8-DKYjR"
      },
      "source": [
        "## Q2. (50 marks)\n",
        "### Implement the idea in paper ***A Neural Probabilistic Language Model*** (https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) to train a trigram model. We will use the brown corpus in nltk package as the dataset. Train the model for 5 epoches and print out the training loss, training accuracy, testing loss, and testing accuracy. You can use these codes to download the corpus:\n",
        "\n",
        "```\n",
        "import nltk\n",
        "nltk.download(\"brown\")\n",
        "from nltk.corpus import brown\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iybz52C7-PRu"
      },
      "outputs": [],
      "source": [
        "# 1. create brown corpus again with all words\n",
        "\n",
        "# 2. create term frequency of the words and vocabulary\n",
        "\n",
        "# 3. creating training and dev set\n",
        "\n",
        "# 4. define Trigram Neural Network Model\n",
        "\n",
        "# 5. using negative log-likelihood loss\n",
        "\n",
        "# ------------------------- TRAIN & SAVE MODEL ------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import time\n",
        "import multiprocessing\n",
        "\n",
        "nltk.download(\"brown\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXPsN-GwLPkN",
        "outputId": "c19679ba-c547-4031-e359-7047c07a4bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = 2\n",
        "HIDDEN_DIM = 100\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 5\n",
        "UNK_SYMBOL = \"<UNK>\"\n",
        "MIN_FREQ = 5\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_WORKERS = multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "XVUmTxEy5uK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and preprocess the Brown corpus\n",
        "corpus = [word.lower() for para in brown.paras() for sent in para for word in sent]\n",
        "vocab_freq = Counter(corpus)\n",
        "vocab = {word for word, freq in vocab_freq.items() if freq >= MIN_FREQ}\n",
        "vocab.add(UNK_SYMBOL)\n",
        "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "UNK_ID = word_to_id[UNK_SYMBOL]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTIRbsenLP7Y",
        "outputId": "dbcdcef4-24bf-4e5b-fa25-0ad31666957f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing Brown corpus...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create term frequency and vocabulary\n",
        "def word_to_id_fn(word):\n",
        "    return word_to_id.get(word, UNK_ID)"
      ],
      "metadata": {
        "id": "4GYpAu2ALTBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Creating trigrams\n",
        "data = np.array([[word_to_id_fn(corpus[i]), word_to_id_fn(corpus[i + 1]), word_to_id_fn(corpus[i + 2])]\n",
        "                 for i in range(len(corpus) - 2)])\n",
        "train_data, dev_data = data[:int(0.8 * len(data))], data[int(0.8 * len(data)):]\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(train_data[:, :2]), torch.tensor(train_data[:, 2])),\n",
        "                          batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "dev_loader = DataLoader(TensorDataset(torch.tensor(dev_data[:, :2]), torch.tensor(dev_data[:, 2])),\n",
        "                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXQui35yLUvd",
        "outputId": "7833b268-18d4-495a-f2e1-080eec6a2a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating trigrams...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define Trigram Neural Network Model\n",
        "class TrigramNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * CONTEXT_SIZE, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.to(DEVICE)).view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.log_softmax(self.fc2(x), dim=1)"
      ],
      "metadata": {
        "id": "QEUrkayqLWX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Using negative log-likelihood loss\n",
        "model = TrigramNN(len(vocab), EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "def compute_accuracy(log_probs, labels):\n",
        "    return (log_probs.argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "def evaluate_model(loader):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for context, target in loader:\n",
        "            context, target = context.to(DEVICE), target.to(DEVICE)  # Move to DEVICE\n",
        "            log_probs = model(context)\n",
        "            total_loss += criterion(log_probs, target).item()\n",
        "            total_acc += compute_accuracy(log_probs, target)\n",
        "    return total_acc / len(loader), total_loss / len(loader)"
      ],
      "metadata": {
        "id": "awnYHO_wLYNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
        "best_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for context, target in train_loader:\n",
        "        context, target = context.to(DEVICE), target.to(DEVICE)  # Move to DEVICE\n",
        "        optimizer.zero_grad()\n",
        "        log_probs = model(context)\n",
        "        loss = criterion(log_probs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    dev_acc, dev_loss = evaluate_model(dev_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}: Dev Accuracy: {dev_acc:.4f}, Dev Loss: {dev_loss:.4f}\")\n",
        "\n",
        "    if dev_acc > best_acc:\n",
        "        best_acc = dev_acc\n",
        "        torch.save(model.state_dict(), f\"best_model_epoch_{epoch + 1}.pth\")\n",
        "        print(f\"New best model saved with accuracy: {best_acc:.4f}\")\n",
        "\n",
        "print(\"Training done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fe9bQWqBVSF",
        "outputId": "10338e53-d62a-4f9d-9388-5be2d375344a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing Brown corpus...\n",
            "Creating trigrams...\n",
            "Initializing model...\n",
            "--- Training starts ---\n",
            "Epoch 1/5: Dev Accuracy: 0.1367, Dev Loss: 5.8231\n",
            "New best model saved with accuracy: 0.1367\n",
            "Epoch 2/5: Dev Accuracy: 0.1430, Dev Loss: 5.8974\n",
            "New best model saved with accuracy: 0.1430\n",
            "Epoch 3/5: Dev Accuracy: 0.1458, Dev Loss: 6.0951\n",
            "New best model saved with accuracy: 0.1458\n",
            "Epoch 4/5: Dev Accuracy: 0.1469, Dev Loss: 6.4017\n",
            "New best model saved with accuracy: 0.1469\n",
            "Epoch 5/5: Dev Accuracy: 0.1452, Dev Loss: 6.7481\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP3FIko4-PRv"
      },
      "source": [
        "## Q3. (10 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqUph0a9-PRv"
      },
      "source": [
        "### Call the chatglm-4 API, and write a proper prompt using prompt engineering knowledge to let chatglm perform the task correctly:\n",
        "\n",
        "``Take the last letters of the words and concatenate them.``\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6gW5-O6-PRv",
        "outputId": "23f73247-4620-4654-f3bd-3bc49c6a8bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zhipuai\n",
            "  Downloading zhipuai-2.1.5.20230904-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cachetools>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (5.5.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core>=2.14.6 in /usr/local/lib/python3.10/dist-packages (from zhipuai) (2.23.4)\n",
            "Collecting pyjwt<2.9.0,>=2.8.0 (from zhipuai)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->zhipuai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->zhipuai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.9.0->zhipuai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->zhipuai) (1.2.2)\n",
            "Downloading zhipuai-2.1.5.20230904-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyjwt, zhipuai\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.10.0\n",
            "    Uninstalling PyJWT-2.10.0:\n",
            "      Successfully uninstalled PyJWT-2.10.0\n",
            "Successfully installed pyjwt-2.8.0 zhipuai-2.1.5.20230904\n"
          ]
        }
      ],
      "source": [
        "!pip install zhipuai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-0asAjt-PRv"
      },
      "outputs": [],
      "source": [
        "words_list= ['Linius Victor', 'strawberry cake', 'Nice headshot', 'Cristiano Ronaldo', 'Brawl Star', 'Natural Language Processing']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zhipuai\n",
        "from zhipuai import ZhipuAI\n",
        "\n",
        "client = ZhipuAI(api_key=\"6026d5961c40106882cd6848016ed219.06LGFZj3PNth6GmI\")\n",
        "\n",
        "# Define the task\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (\n",
        "            \"Your task is to extract and concatenate the **last letter** of **each word** from the following phrases.\\n\"\n",
        "            \"Make sure to:\\n\"\n",
        "            \"1. **Extract the last letter** of every word in the phrase.\\n\"\n",
        "            \"2. **Concatenate** all extracted letters into one continuous string.\\n\"\n",
        "            \"3. Return a **list** of results for each phrase.\\n\\n\"\n",
        "            \"**Example:**\\n\"\n",
        "            \"- Phrase: 'Big Red Car'\\n\"\n",
        "            \"- Extraction: 'g' from 'Big', 'd' from 'Red', 'r' from 'Car'\\n\"\n",
        "            \"- Result: 'gdr'\\n\\n\"\n",
        "            \"**Phrases to process:**\\n\"\n",
        "            \"1. Linius Victor\\n\"\n",
        "            \"2. strawberry cake\\n\"\n",
        "            \"3. Nice headshot\\n\"\n",
        "            \"4. Cristiano Ronaldo\\n\"\n",
        "            \"5. Brawl Star\\n\"\n",
        "            \"6. Natural Language Processing\\n\\n\"\n",
        "            \"Please return a **list** of concatenated last letters for each phrase.\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "# Call the API\n",
        "response = client.chat.completions.create(\n",
        "    model=\"glm-4-plus\",\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "# Output the result\n",
        "output_message = response.choices[0].message.content.strip()\n",
        "print(output_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYFs6pEJMpID",
        "outputId": "24077458-65f2-46d4-fe6a-05720cf727f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To accomplish this task, I will follow the steps outlined:\n",
            "\n",
            "1. Extract the last letter of each word in the phrase.\n",
            "2. Concatenate all extracted letters into one continuous string.\n",
            "3. Return a list of results for each phrase.\n",
            "\n",
            "Here is the list of concatenated last letters for each provided phrase:\n",
            "\n",
            "1. **Linius Victor**\n",
            "   - 'Linius' -> 's'\n",
            "   - 'Victor' -> 'r'\n",
            "   - Result: 'sr'\n",
            "\n",
            "2. **strawberry cake**\n",
            "   - 'strawberry' -> 'y'\n",
            "   - 'cake' -> 'e'\n",
            "   - Result: 'ye'\n",
            "\n",
            "3. **Nice headshot**\n",
            "   - 'Nice' -> 'e'\n",
            "   - 'headshot' -> 't'\n",
            "   - Result: 'et'\n",
            "\n",
            "4. **Cristiano Ronaldo**\n",
            "   - 'Cristiano' -> 'o'\n",
            "   - 'Ronaldo' -> 'o'\n",
            "   - Result: 'oo'\n",
            "\n",
            "5. **Brawl Star**\n",
            "   - 'Brawl' -> 'l'\n",
            "   - 'Star' -> 'r'\n",
            "   - Result: 'lr'\n",
            "\n",
            "6. **Natural Language Processing**\n",
            "   - 'Natural' -> 'l'\n",
            "   - 'Language' -> 'e'\n",
            "   - 'Processing' -> 'g'\n",
            "   - Result: 'leg'\n",
            "\n",
            "**Final List:**\n",
            "```python\n",
            "['sr', 'ye', 'et', 'oo', 'lr', 'leg']\n",
            "```\n",
            "\n",
            "This list contains the concatenated last letters for each of the given phrases.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}